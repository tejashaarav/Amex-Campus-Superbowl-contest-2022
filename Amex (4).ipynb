{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2 style = \"BentonSans-Light,helvetica,sans-serif; color: #2196f3\"> Code Submission by Team Thunder_BUDDIES </h2>**\n",
    "\n",
    "                                                                                - Siva Sankar S\n",
    "                                                                                - Tejashaarav S\n",
    "                                                                                - Keerthy Babu D\n",
    "                                                                                \n",
    "This is notebook is a compilation of all the approaches tried by our team. \n",
    "\n",
    "\n",
    "|Name|College|Course|Batch Year|Roll no.|Mobile Numer|Email-ID|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Siva Sankar S| IIT-Madras|B.Tech in Chemical Engineering|2024|CH20B103|9283217898|ch20b103@smail.iitm.ac.in|\n",
    "|Keerthy Babu D|\tIIT Madras\t|B.Tech in Chemical Engineering\t|2024\t|CH20B059\t|7483013164\t|ch20b059@smail.iitm.ac.in|\n",
    "|Tejashaarav S|\tIIT Madras\t|B.Tech in Chemical Engineering\t|2024\t|CH20B107\t|9080783016\t|ch20b107@smail.iitm.ac.in|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"first-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint \n",
    "\n",
    "from sklearn.impute import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clear_output()\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and analysis <a class = \"anchor\" id =\"dpa\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.read_csv('Modelling_Students/train_allx.csv')\n",
    "df_y = pd.read_csv('Modelling_Students/train_y.csv')\n",
    "df_val = pd.read_csv('Modelling_Students/val_allx.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshuffling the data to remove any bias in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "df_X = df_X.sample(frac=1.)\n",
    "df_y = df_y.reindex(df_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy class imbalance:\n",
    "* The data is highly skewed with label 0 being the majority class (74.11 %) and label 1 being minority class (7.52 %).\n",
    "* To overcome this class imbalance problem, we tried to use SMOTE algorithm to artificially generate data points.\n",
    "* We also tried under-sampling majority class and over-sampling the minority classes.\n",
    "* Handling the imbalance in given data doesnt necessarily improve the accuracy of our predictions which is the major scoring metric for this competition\n",
    "* We observed that changing the original distribution improved F1 scores but overall accuracy is also reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of Default Flag label 0:\t 340085\t (74.11%)\n",
      "Total count of Default Flag label 1:\t 34501\t (7.52%)\n",
      "Total count of Default Flag label 2:\t 46424\t (10.12%)\n",
      "Total count of Default Flag label 3:\t 37903\t (8.26%)\n"
     ]
    }
   ],
   "source": [
    "n = len(df_y)\n",
    "for (label,),count in df_y.value_counts().sort_index().items():\n",
    "    print(f'Total count of Default Flag label {label}:\\t {count}\\t ({round(count/n*100,2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Count'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_y.value_counts().sort_index().reset_index(name='Count').plot(kind='pie',x='Default_Flag',y='Count',\n",
    "                                                            explode=[0,0.2,0.2,0.2],autopct=lambda p:f'{p:.2f}%',\n",
    "                                                            shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large number of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts\n",
      "Column Name: D_17 \t-------\t Count: 458268\t-------\t Percentage: 99.86 %\n",
      "Column Name: D_38 \t-------\t Count: 458086\t-------\t Percentage: 99.82 %\n",
      "Column Name: D_43 \t-------\t Count: 456286\t-------\t Percentage: 99.43 %\n",
      "Column Name: D_39 \t-------\t Count: 455235\t-------\t Percentage: 99.2 %\n",
      "Column Name: D_96 \t-------\t Count: 455235\t-------\t Percentage: 99.2 %\n",
      "Column Name: B_7 \t-------\t Count: 454808\t-------\t Percentage: 99.11 %\n",
      "Column Name: D_73 \t-------\t Count: 454674\t-------\t Percentage: 99.08 %\n",
      "Column Name: B_22 \t-------\t Count: 452771\t-------\t Percentage: 98.66 %\n",
      "Column Name: D_114 \t-------\t Count: 442518\t-------\t Percentage: 96.43 %\n",
      "Column Name: D_132 \t-------\t Count: 442518\t-------\t Percentage: 96.43 %\n",
      "Column Name: D_12 \t-------\t Count: 442518\t-------\t Percentage: 96.43 %\n",
      "Column Name: D_80 \t-------\t Count: 442518\t-------\t Percentage: 96.43 %\n",
      "Column Name: D_97 \t-------\t Count: 442518\t-------\t Percentage: 96.43 %\n",
      "Column Name: R_8 \t-------\t Count: 431960\t-------\t Percentage: 94.13 %\n",
      "Column Name: B_26 \t-------\t Count: 431589\t-------\t Percentage: 94.05 %\n",
      "Column Name: D_110 \t-------\t Count: 409597\t-------\t Percentage: 89.25 %\n",
      "Column Name: R_7 \t-------\t Count: 407770\t-------\t Percentage: 88.86 %\n",
      "Column Name: D_11 \t-------\t Count: 407265\t-------\t Percentage: 88.75 %\n",
      "Column Name: D_95 \t-------\t Count: 407153\t-------\t Percentage: 88.72 %\n",
      "Column Name: D_105 \t-------\t Count: 407150\t-------\t Percentage: 88.72 %\n",
      "Column Name: D_48 \t-------\t Count: 406331\t-------\t Percentage: 88.54 %\n",
      "Column Name: D_141 \t-------\t Count: 399003\t-------\t Percentage: 86.95 %\n",
      "Column Name: D_142 \t-------\t Count: 378598\t-------\t Percentage: 82.5 %\n",
      "Column Name: D_10 \t-------\t Count: 343295\t-------\t Percentage: 74.81 %\n",
      "Column Name: D_68 \t-------\t Count: 325932\t-------\t Percentage: 71.02 %\n",
      "Column Name: D_92 \t-------\t Count: 262235\t-------\t Percentage: 57.14 %\n",
      "Column Name: D_31 \t-------\t Count: 245602\t-------\t Percentage: 53.52 %\n",
      "Column Name: D_106 \t-------\t Count: 244734\t-------\t Percentage: 53.33 %\n",
      "Column Name: B_29 \t-------\t Count: 244471\t-------\t Percentage: 53.27 %\n",
      "Column Name: D_133 \t-------\t Count: 213837\t-------\t Percentage: 46.6 %\n",
      "Column Name: S_23 \t-------\t Count: 183858\t-------\t Percentage: 40.06 %\n",
      "Column Name: D_89 \t-------\t Count: 134322\t-------\t Percentage: 29.27 %\n",
      "Column Name: S_19 \t-------\t Count: 117166\t-------\t Percentage: 25.53 %\n",
      "Column Name: D_8 \t-------\t Count: 95123\t-------\t Percentage: 20.73 %\n",
      "Column Name: S_25 \t-------\t Count: 84970\t-------\t Percentage: 18.52 %\n",
      "Column Name: S_7 \t-------\t Count: 84970\t-------\t Percentage: 18.52 %\n",
      "Column Name: D_64 \t-------\t Count: 58953\t-------\t Percentage: 12.85 %\n",
      "Column Name: D_40 \t-------\t Count: 57992\t-------\t Percentage: 12.64 %\n",
      "Column Name: D_22 \t-------\t Count: 48348\t-------\t Percentage: 10.54 %\n",
      "Column Name: D_99 \t-------\t Count: 30377\t-------\t Percentage: 6.62 %\n",
      "Column Name: R_5 \t-------\t Count: 28736\t-------\t Percentage: 6.26 %\n",
      "Column Name: D_6 \t-------\t Count: 22295\t-------\t Percentage: 4.86 %\n",
      "Column Name: D_56 \t-------\t Count: 22295\t-------\t Percentage: 4.86 %\n",
      "Column Name: P_1 \t-------\t Count: 22220\t-------\t Percentage: 4.84 %\n",
      "Column Name: D_4 \t-------\t Count: 12807\t-------\t Percentage: 2.79 %\n",
      "Column Name: D_44 \t-------\t Count: 10037\t-------\t Percentage: 2.19 %\n",
      "Column Name: D_19 \t-------\t Count: 9012\t-------\t Percentage: 1.96 %\n",
      "Column Name: D_34 \t-------\t Count: 6365\t-------\t Percentage: 1.39 %\n",
      "Column Name: D_88 \t-------\t Count: 6365\t-------\t Percentage: 1.39 %\n",
      "Column Name: D_9 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_32 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_91 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_124 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_102 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_20 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_54 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_47 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_120 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_49 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_70 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_83 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_26 \t-------\t Count: 6017\t-------\t Percentage: 1.31 %\n",
      "Column Name: D_65 \t-------\t Count: 4196\t-------\t Percentage: 0.91 %\n",
      "Column Name: B_14 \t-------\t Count: 4091\t-------\t Percentage: 0.89 %\n",
      "Column Name: D_90 \t-------\t Count: 4086\t-------\t Percentage: 0.89 %\n",
      "Column Name: P_2 \t-------\t Count: 2969\t-------\t Percentage: 0.65 %\n",
      "Column Name: D_23 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_55 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_18 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_108 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_84 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_7 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_135 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_139 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_87 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_51 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_123 \t-------\t Count: 2830\t-------\t Percentage: 0.62 %\n",
      "Column Name: D_67 \t-------\t Count: 2795\t-------\t Percentage: 0.61 %\n",
      "Column Name: S_26 \t-------\t Count: 1767\t-------\t Percentage: 0.39 %\n",
      "Column Name: S_11 \t-------\t Count: 1734\t-------\t Percentage: 0.38 %\n",
      "Column Name: D_50 \t-------\t Count: 1701\t-------\t Percentage: 0.37 %\n",
      "Column Name: D_69 \t-------\t Count: 1701\t-------\t Percentage: 0.37 %\n",
      "Column Name: B_21 \t-------\t Count: 1563\t-------\t Percentage: 0.34 %\n",
      "Column Name: S_8 \t-------\t Count: 1421\t-------\t Percentage: 0.31 %\n",
      "Column Name: D_126 \t-------\t Count: 1240\t-------\t Percentage: 0.27 %\n",
      "Column Name: D_74 \t-------\t Count: 1240\t-------\t Percentage: 0.27 %\n",
      "Column Name: D_100 \t-------\t Count: 1240\t-------\t Percentage: 0.27 %\n",
      "Column Name: D_98 \t-------\t Count: 1216\t-------\t Percentage: 0.26 %\n",
      "Column Name: D_59 \t-------\t Count: 1190\t-------\t Percentage: 0.26 %\n",
      "Column Name: B_6 \t-------\t Count: 612\t-------\t Percentage: 0.13 %\n",
      "Column Name: B_8 \t-------\t Count: 612\t-------\t Percentage: 0.13 %\n",
      "Column Name: S_16 \t-------\t Count: 44\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_40 \t-------\t Count: 40\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_17 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_31 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_13 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_33 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_3 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_12 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: D_121 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: D_107 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_42 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: D_79 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_5 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_10 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_18 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: D_93 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: D_101 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n",
      "Column Name: B_9 \t-------\t Count: 31\t-------\t Percentage: 0.01 %\n"
     ]
    }
   ],
   "source": [
    "print('NaN counts')\n",
    "for col_name,nan_count in df_X.isna().sum().sort_values(ascending=False).items():\n",
    "    if nan_count:\n",
    "        print(f'Column Name: {col_name} \\t-------\\t Count: {nan_count}\\t-------\\t Percentage: {round(nan_count/n*100,2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the NaNs\n",
    "* In order to find optimal approach to handling NaNs, we tried out various strategies and compared the accuracies we got with a cross validated control model\n",
    "* We also discovered that Histogram-based Gradient Boosting Classification Tree is robust with NaNs. Sklearn implementation of the algorithm have native support for handling NaNs.\n",
    "> \"During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain\" -  Sklearn Documentaion on Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validated score for Histogram based gradient boosting classification tree:\t 0.8146184532085133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = HistGradientBoostingClassifier()\n",
    "X = pd.get_dummies(df_X)\n",
    "score = cross_val_score(clf,X,df_y.iloc[:,0],n_jobs=3)\n",
    "print(f'Mean cross validated score for Histogram based gradient boosting classification tree:\\t {score.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some strategies we tried out and tested are:\n",
    "    * Mean Imputation with and without NaN indicator\n",
    "    * Median Imputation with and without NaN indicator\n",
    "    * KNN based imputation with and without NaN indicator\n",
    "    * Constant fill with and without NaN indicator\n",
    "\n",
    "* We observed that most frequent fill without NaN indicator and constant fill with value: 0 is the most optimal upon testing on multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(n=5000):\n",
    "    \"\"\"\n",
    "    Returns random samples from the main data. Used for experiments on the data\n",
    "    \"\"\"\n",
    "    X = pd.get_dummies(df_X) \n",
    "    Samples_X = X.sample(5000)\n",
    "    Samples_y = df_y.loc[Samples_X.index]\n",
    "    return Samples_X,Samples_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validated score without handling NaN:\t 0.7987997838607549\n",
      "\n",
      "Mean cross validated score for mean without NaN indicator:\t 0.802200064188843\n",
      "Mean cross validated score for mean with NaN indicator:\t 0.8033999442608476\n",
      "\n",
      "\n",
      "Mean cross validated score for median without NaN indicator:\t 0.8030005043409085\n",
      "Mean cross validated score for median with NaN indicator:\t 0.8036012245330045\n",
      "\n",
      "\n",
      "Mean cross validated score for most_frequent without NaN indicator:\t 0.806200464588955\n",
      "Mean cross validated score for most_frequent with NaN indicator:\t 0.8032013045170078\n",
      "\n",
      "\n",
      "Mean cross validated score for constant without NaN indicator:\t 0.8053991842687885\n",
      "Mean cross validated score for constant with NaN indicator:\t 0.8049994243648269\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "strategies = ['mean','median','most_frequent','constant']\n",
    "\n",
    "Samples_X,Samples_y = sample()\n",
    "\n",
    "clf = HistGradientBoostingClassifier() # HistGradientBoostingClassifier is used as reference model\n",
    "score = cross_val_score(clf,Samples_X,Samples_y.iloc[:,0],n_jobs=3,cv=3)    \n",
    "print(f'Mean cross validated score without handling NaN:\\t {score.mean()}\\n')\n",
    "\n",
    "for strategy in strategies:\n",
    "    imputator = SimpleImputer(strategy=strategy,add_indicator=False).fit(X) # No NaN indicator\n",
    "    Samples_X_testing = imputator.transform(Samples_X)\n",
    "    clf = HistGradientBoostingClassifier() \n",
    "    score = cross_val_score(clf,Samples_X_testing,Samples_y.iloc[:,0],n_jobs=3,cv=3) \n",
    "    print(f'Mean cross validated score for {strategy} without NaN indicator:\\t {score.mean()}') \n",
    "\n",
    "    imputator = SimpleImputer(strategy=strategy,add_indicator=True).fit(Samples_X) # With NaN indicator\n",
    "    Samples_X_testing = imputator.transform(Samples_X)\n",
    "    clf = HistGradientBoostingClassifier() \n",
    "    score = cross_val_score(clf,Samples_X_testing,Samples_y.iloc[:,0],n_jobs=3,cv=3) \n",
    "    print(f'Mean cross validated score for {strategy} with NaN indicator:\\t {score.mean()}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does dropping features with high count of NaN's improve score?\n",
    "* We tried dropping features that have a high count of NaN values and we did observe that it didn't improve the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 15 columns to be dropped.\n"
     ]
    }
   ],
   "source": [
    "cutoff = 0.9\n",
    "Samples_X,Samples_y = sample()\n",
    "to_drop = (Samples_X.isna().sum().sort_values(ascending=False)/len(Samples_X)>cutoff)\n",
    "to_drop_cols = to_drop.loc[to_drop].index\n",
    "print(f'Total of {len(to_drop_cols)} columns to be dropped.')\n",
    "Samples_X_testing = Samples_X.drop(to_drop_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validated score without frequent NaN features dropped:\t 0.8042000243408701\n",
      "\n",
      "\n",
      "Mean cross validated score with frequent NaN features dropped:\t 0.8037992641567726\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Samples_X_testing = SimpleImputer(strategy='most_frequent',add_indicator=False).fit_transform(Samples_X_testing) \n",
    "Samples_X = SimpleImputer(strategy='most_frequent',add_indicator=False).fit_transform(Samples_X) \n",
    "clf = HistGradientBoostingClassifier() \n",
    "score_testing = cross_val_score(clf,Samples_X_testing,Samples_y.iloc[:,0],n_jobs=3,cv=3) \n",
    "score_control = cross_val_score(clf,Samples_X,Samples_y.iloc[:,0],n_jobs=3,cv=3) \n",
    "print(f'Mean cross validated score without frequent NaN features dropped:\\t {score_control.mean()}\\n\\n')\n",
    "print(f'Mean cross validated score with frequent NaN features dropped:\\t {score_testing.mean()}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data\n",
    "* For tree based models, scaling the data makes no difference as they are not sensitive to variance in the data.\n",
    "* Scaling the data makes learning easier for models that uses gradient descends.\n",
    "* Based on our experiments, quantile transformer with linear distribution worked the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score without scaling (Tree based):\t 0.7484000000000001\n",
      "Score without scaling (Gradient based):\t 0.5508\n",
      "\n",
      "\n",
      "Score with Standard Scaler scaling (Tree based):\t 0.7448\n",
      "Score with Standard Scaler scaling (Gradient based):\t 0.618\n",
      "\n",
      "\n",
      "Score with Robust Scaler scaling (Tree based):\t 0.7434\n",
      "Score with Robust Scaler scaling (Gradient based):\t 0.6178\n",
      "\n",
      "\n",
      "Score with Quantile Transformer Uniform scaling (Tree based):\t 0.7449999999999999\n",
      "Score with Quantile Transformer Uniform scaling (Gradient based):\t 0.6836\n",
      "\n",
      "\n",
      "Score with Quantile Transformer Gaussian scaling (Tree based):\t 0.7409999999999999\n",
      "Score with Quantile Transformer Gaussian scaling (Gradient based):\t 0.6178\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tree_based_model = DecisionTreeClassifier\n",
    "gradient_based_model = MLPClassifier\n",
    "\n",
    "scalers = [('Standard Scaler',StandardScaler()),('Robust Scaler',RobustScaler()),('Quantile Transformer Uniform',QuantileTransformer()),('Quantile Transformer Gaussian',QuantileTransformer(output_distribution='normal'))]\n",
    "\n",
    "Samples_X, Samples_y = sample(10000)\n",
    "Samples_X = SimpleImputer(strategy='most_frequent').fit_transform(Samples_X)\n",
    "\n",
    "# Without any scaling\n",
    "tree_score = cross_val_score(tree_based_model(),Samples_X,Samples_y.iloc[:,0],n_jobs=3,cv=10).mean()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "    grad_score = cross_val_score(gradient_based_model(),Samples_X,Samples_y.iloc[:,0],n_jobs=3,cv=10).mean()\n",
    "print(f'Score without scaling (Tree based):\\t {tree_score}')\n",
    "print(f'Score without scaling (Gradient based):\\t {grad_score}\\n\\n')\n",
    "\n",
    "# Testing the Scalers:\n",
    "for name,scaler in  scalers:\n",
    "    Samples_X_testing = scaler.fit_transform(Samples_X)\n",
    "    tree_score = cross_val_score(tree_based_model(),Samples_X_testing,Samples_y.iloc[:,0],n_jobs=3,cv=10).mean()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "        grad_score = cross_val_score(gradient_based_model(),Samples_X,Samples_y.iloc[:,0],n_jobs=3,cv=10).mean()\n",
    "    print(f'Score with {name} scaling (Tree based):\\t {tree_score}')\n",
    "    print(f'Score with {name} scaling (Gradient based):\\t {grad_score}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final set of preprocessing:\n",
    "After experimenting with a lot of preprocessing methods this is the flow we have chosen for the final set of preprocessing.\n",
    "* Simple Imputation with most frequent value\n",
    "* Quantile Transformer to uniform distribution\n",
    "* One hot encoding the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy='most_frequent',add_indicator=False)\n",
    "        self.scaler = QuantileTransformer()\n",
    "        self.CatCols = None\n",
    "        self.Cols = None\n",
    "        \n",
    "    def fit(self,df):\n",
    "        self.CatCols = df.select_dtypes(exclude=['float','int']).columns\n",
    "        df = pd.get_dummies(df,columns=self.CatCols,prefix='ohe')\n",
    "        self.Cols = df.columns\n",
    "        df = self.imputer.fit_transform(df)\n",
    "        self.scaler.fit(df)\n",
    "        return self\n",
    "\n",
    "    def transform(self,df):\n",
    "            df = pd.get_dummies(df,columns=self.CatCols,prefix='ohe')\n",
    "            df= self.imputer.transform(df)\n",
    "            df = self.scaler.transform(df)\n",
    "            df = pd.DataFrame(df,columns=self.Cols)\n",
    "            cats = df.columns[df.columns.str.startswith('ohe_')]\n",
    "            for cat in cats: df.loc[:,cat] = df.loc[:,cat].astype(int) \n",
    "            return df\n",
    "\n",
    "    def fit_transform(self,df):\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "    def __call__(self,df):\n",
    "         return self.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt on feature extraction using unsupervised denoising auto-encoder\n",
    "* We were hunting for a suitable auto-encoder for this dataset and we got to know about unsupervised auto-encoder\n",
    "* The core idea behind this auto-encoder is to add noise to tabular data and build a model to predict the original data.\n",
    "* The layer wise outputs of the model are concatenated to form new feature set for a second model.\n",
    "* The main advantage of this approach is that, in the representation learning no labels are required. This means we can use both labelled train data as well as unlabelled val data.\n",
    "References:\n",
    "* https://www.kaggle.com/code/springmanndaniel/1st-place-turn-your-data-into-daeta/report\n",
    "* https://www.kaggle.com/c/tabular-playground-series-jan-2021/discussion/216037\n",
    "\n",
    "\n",
    "|![DAEs](https://s10.gifyu.com/images/deepstack_input_output_color.png)|\n",
    "|:--:|\n",
    "| *Denoising AutoEncoder used by Danzel, Winning solution, Tabular Playground series*|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "preprocess = Preprocess()\n",
    "X = preprocess.fit_transform(df_X)\n",
    "X_val = preprocess.transform(df_val)\n",
    "\n",
    "# Sampling seperate data to test the trained data representation\n",
    "df = pd.concat([pd.DataFrame(X),df_y],axis=1)\n",
    "clf_data = pd.concat([G.sample(3000) for n,G in df.groupby('Default_Flag')],axis=0).sample(frac=1.)\n",
    "X_clf = clf_data.drop('Default_Flag',axis=1)\n",
    "y_clf = clf_data['Default_Flag']\n",
    "X_repr_lr = np.concatenate([X,X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pytorch dataloaders\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "n = int(len(df_X)*0.9)\n",
    "X_train = X_repr_lr[:n]\n",
    "X_test = X_repr_lr[n:]\n",
    "train_data = DataSet(X_train)\n",
    "test_data = DataSet(X_test)\n",
    "clf_dataset = DataSet(X_clf.values)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=1024, shuffle=True,num_workers=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1024,num_workers=1,shuffle=True)\n",
    "clf_dataloader = DataLoader(clf_dataset, batch_size=1024,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_features(X,y):\n",
    "    \"\"\"\n",
    "    Used to evaluate the feature quality. GaussianNB is used because of its fast training time\n",
    "    \"\"\"\n",
    "    return cross_val_score(GaussianNB(),X,y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Representation_Learner(pl.LightningModule):\n",
    "    def __init__(self,sigmas = [0.1,0.5,0.7,0.9,0.1],\n",
    "                        dev='cuda',min_swaps_frac=0.1, \n",
    "                            max_swaps_frac=0.4,max_noise_frac=0.1):\n",
    "        super().__init__()\n",
    "        self.sigmas = sigmas\n",
    "        self.min_swaps_frac = min_swaps_frac\n",
    "        self.max_swaps_frac = max_swaps_frac\n",
    "        self.max_noise_frac = max_noise_frac\n",
    "        self.dev = dev\n",
    "        self.L1 = nn.Linear(215,256) \n",
    "        self.L2 = nn.Linear(256,256)\n",
    "        self.L3 = nn.Linear(256,256)\n",
    "        self.L4 = nn.Linear(256,215)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        f1 = self.lrelu(self.L1(x))\n",
    "        f2 = self.lrelu(self.L2(f1))\n",
    "        f3 = self.lrelu(self.L3(f2))\n",
    "        x = self.lrelu(self.L4(f3))\n",
    "        return x,torch.concat([f1,f2,f3],dim=-1)\n",
    "    \n",
    "    def random_noise_feature(self,x,clone=False):\n",
    "        \"\"\"\n",
    "        Select random number of features and add noise feature wise\n",
    "        \"\"\"\n",
    "        if clone:\n",
    "            x = torch.clone(x)\n",
    "        sigma = random.choice(self.sigmas)\n",
    "        p = random.randint(0,int(x.shape[1]*self.max_noise_frac)) # Number of features to add noise\n",
    "        feats = torch.randint(0,x.shape[1]-1,(p,))\n",
    "        noise = torch.rand((x.shape[0],p),device=self.dev) * sigma\n",
    "        x[:,feats] += noise\n",
    "        return x\n",
    "    \n",
    "    def random_noise(self,z,clone=False):\n",
    "        \"\"\"\n",
    "        Select random entries and add noise\n",
    "        \"\"\"\n",
    "        if clone:\n",
    "            z = torch.clone(z)\n",
    "        sigma = random.choice(self.sigmas)\n",
    "        n = random.randint(0,int(z.numel() * self.max_noise_frac))\n",
    "        nr = torch.randint(0,z.shape[0]-1,(n,))\n",
    "        nf = torch.randint(0,z.shape[1]-1,(n,)) # Number of features to add noise\n",
    "        c = torch.stack([nr,nf])\n",
    "        noise = torch.rand((n,),device=self.dev) * sigma\n",
    "        z[c[0],c[1]]  +=  noise\n",
    "        return z\n",
    "    \n",
    "    def random_swap(self,z):\n",
    "        \"\"\"\n",
    "        Select random elements from table and swap them\n",
    "        \"\"\"\n",
    "        s = z.numel()\n",
    "        n = random.randint(int(s* self.min_swaps_frac),int(s* self.max_swaps_frac)) # number of swaps\n",
    "\n",
    "        nr1 = torch.randint(0,z.shape[0]-1,(n,))\n",
    "        nf1 = torch.randint(0,z.shape[1]-1,(n,)) \n",
    "        c1 = torch.stack([nr1,nf1])\n",
    "\n",
    "\n",
    "        nr2 = torch.randint(0,z.shape[0]-1,(n,))\n",
    "        nf2 = torch.randint(0,z.shape[1]-1,(n,)) \n",
    "        c2 = torch.stack([nr2,nf2])\n",
    "\n",
    "        z[c1[0],c1[1]], z[c2[0],c2[1]] = z[c2[0],c2[1]], z[c1[0],c1[1]]\n",
    "        return z\n",
    "    \n",
    "    def corrupt(self,x):\n",
    "        x = self.random_noise_feature(x) \n",
    "        x = self.random_noise(x)\n",
    "        x = self.random_swap(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def training_step(self,x, batch_idx):\n",
    "        x = x.float()\n",
    "        y = torch.clone(x) # y is original data, x is corrupted data\n",
    "        x = self.corrupt(x)\n",
    "        yhat = self.forward(x)[0]\n",
    "        var = torch.var(x-y) # Variance of added corruption\n",
    "        self.log('train_var',var)\n",
    "        loss = torch.nn.functional.mse_loss(yhat,y)\n",
    "        self.log('train_mse',loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,x, batch_idx):\n",
    "        x = x.float()\n",
    "        y = torch.clone(x) # y is original data, x is corrupted data\n",
    "        x = self.corrupt(x)\n",
    "        yhat = self.forward(x)[0]\n",
    "        var = torch.var(x-y) # Variance of added corruption\n",
    "        self.log('val_var',var)\n",
    "        loss = torch.nn.functional.mse_loss(yhat,y)\n",
    "        self.log('val_mse',loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Evaluate the features\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            X_trans = [self.forward(x.to('cuda'))[1].cpu() for x in clf_dataloader]\n",
    "        X_trans = torch.concat(X_trans).numpy()\n",
    "        score = eval_features(X_trans,y_clf)\n",
    "        self.log('val_acc',score)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-6,weight_decay=1e-4) # highly regularized\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_learner = Representation_Learner()\n",
    "print(f'Model score with original set of features: {eval_features(X_clf,y_clf)}')\n",
    "\n",
    "logger = WandbLogger(name='DeepStackv2',project='Amex')\n",
    "logger.watch(repr_learner)\n",
    "\n",
    "checkpoint = ModelCheckpoint(save_top_k=3,monitor='val_mse',mode='min',dirpath='Models',filename='DeepStack_Model-{epoch:04d}-{val_mse:.10f}-{val_acc:.5f}')\n",
    "trainer = pl.Trainer(max_epochs=20000,accelerator='gpu',limit_train_batches=8000,limit_val_batches=1500,logger=logger,callbacks=[checkpoint],enable_progress_bar = False)\n",
    "\n",
    "trainer.fit(model=repr_learner, train_dataloaders=train_dataloader,val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![Screenshot-2022-10-22-at-10-18-47-AM.png](https://i.postimg.cc/fTN6kxHR/Screenshot-2022-10-22-at-10-18-47-AM.png)](https://postimg.cc/SjgZvzFB) |\n",
    "| :--: |\n",
    "| *Despite high hopes, the validation accuracy didnt improve. We believe the main reason for this lowered accuracy is the presence of too much NaNs. The dataset's increased dimension could be another factor. We also faced practical issue of fitting the transformed features in the memory!*|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling of NaNs using own implementation of Denoising Diffusion Probabilistic Models (DDPMs)\n",
    "* In recent times, Denoising Diffusion Probabilistic Models have shown some stunning performance in image generation.\n",
    "* Modern image generation models like DALL-E, Stable Diffusion uses DDPMs for state of the art image generation.\n",
    "* We implemented 1D version of DDPMs, DDPMs can be used to remove noise in the data as well as generate literally infinite number of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPS_Model(pl.LightningModule):\n",
    "    def __init__(self,timesteps=250,dev='cuda'):\n",
    "        super().__init__()\n",
    "        self.default_t = timesteps-1\n",
    "        self.dev = dev\n",
    "        self.timesteps = timesteps\n",
    "        self.embeddings = torch.nn.Embedding(timesteps,16)\n",
    "        self.B1 = nn.BatchNorm1d(16)\n",
    "        self.B2 = nn.BatchNorm1d(1)\n",
    "        self.C1 = nn.Conv1d(1,16,5,padding='same')\n",
    "        self.C2 = nn.Conv1d(16,16,5,padding='same')\n",
    "        self.C3 = nn.Conv1d(16,1,5,padding='same')\n",
    "        self.M1 = nn.MaxPool1d(2,2)\n",
    "        self.L1 = nn.Linear(215+8,512) \n",
    "        self.L2 = nn.Linear(512,256)\n",
    "        self.L3 = nn.Linear(256,64)\n",
    "        self.L4 = nn.Linear(64,256)\n",
    "        self.L5 = nn.Linear(256,215)\n",
    "    \n",
    "    def forward(self,x,t=None):\n",
    "        if t is None:\n",
    "            t = torch.ones((x.shape[0],), device=self.dev, dtype=torch.long) * self.default_t\n",
    "        embeds = self.embeddings(t)\n",
    "        e = self.B1(embeds).reshape(embeds.shape[0],1,-1)\n",
    "        e1 = self.C1(e)\n",
    "        e2 = self.C2(e1) + e1\n",
    "        e3 = self.C3(e2) \n",
    "        e4 = self.M1(e3)  \n",
    "        e4 = self.B2(e4)\n",
    "        x = torch.cat([x,e4.squeeze()],-1).float()\n",
    "        x = torch.relu(self.L1(x))\n",
    "        x = torch.relu(self.L2(x))\n",
    "        f = torch.relu(self.L3(x))\n",
    "        x = torch.relu(self.L4(f))\n",
    "        x = torch.sigmoid(self.L5(x))\n",
    "        return x,f\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self.forward(batch)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DiffusionModel(pl.LightningModule):\n",
    "    def __init__(self,timesteps,eps_model,loss=None,dev='cuda') -> None:\n",
    "        super().__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.dev = dev\n",
    "        self.default_t = timesteps-1\n",
    "\n",
    "        self.beta = cosine_beta_schedule(timesteps).to(dev)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = self.alpha.cumprod(-1)\n",
    "        self.a = self.alpha_bar**0.5 \n",
    "        self.b = (1-self.alpha_bar)**0.5\n",
    "        self.alphas_bar_prev = nn.functional.pad(self.alpha_bar[:-1], (1, 0), value=1.0)\n",
    "        self.post_var  = self.beta * (1. - self.alphas_bar_prev) / (1. - self.alpha_bar)\n",
    "\n",
    "        self.eps_model = eps_model.to(dev)\n",
    "        if dev=='cuda':\n",
    "            self.predictor = pl.Trainer(accelerator='gpu')\n",
    "        else:\n",
    "            self.predictor = pl.Trainer(accelerator='cpu')\n",
    "        if loss:\n",
    "            self.loss_fn = loss\n",
    "        else:\n",
    "            self.loss_fn = nn.functional.mse_loss\n",
    "    \n",
    "    def forward(self,xt,t):\n",
    "        return self.eps_model(xt,t)[0]\n",
    "\n",
    "    def q_sample(self,x0,t,noise=None):\n",
    "        # adds noise, xt -> xt+1\n",
    "        if noise is None:\n",
    "            noise = torch.rand_like(x0).to(self.dev)\n",
    "        if torch.is_tensor(t):\n",
    "            t = t.long()\n",
    "        return self.a[t].reshape(-1,1) * x0 + self.b[t].reshape(-1,1) * noise\n",
    "\n",
    "    def p_sample(self,xt,t):\n",
    "        # remove noise, xt -> xt-1\n",
    "        eps = self.forward(xt,t)\n",
    "        a = 1/self.alpha[t].reshape(-1,1)**0.5 * (xt - self.beta[t].reshape(-1,1)*eps/(1-self.alpha_bar[t].reshape(-1,1)**0.5))\n",
    "        b = self.beta[t].reshape(-1,1)\n",
    "        return a + eps*b\n",
    "    \n",
    "    def sample(self,noise=None,steps=None):\n",
    "        if noise is None:\n",
    "            noise = torch.rand_like(self.shape)\n",
    "        if steps is None:\n",
    "            steps = self.timesteps\n",
    "        x = noise\n",
    "        for t_bar in range(steps):\n",
    "            x = self.p_sample(x,self.timesteps-t_bar)\n",
    "        return x\n",
    "        \n",
    "    def training_step(self,x0, batch_idx):\n",
    "        batch_size = x0.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=x0.device, dtype=torch.long)\n",
    "        eps = torch.rand_like(x0)\n",
    "        xt = self.q_sample(x0,t)\n",
    "        eps_pred = self.p_sample(xt,t)\n",
    "        loss = self.loss_fn(eps_pred,eps)\n",
    "        self.log('train_mse',loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,x0, batch_idx):\n",
    "        batch_size = x0.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=x0.device, dtype=torch.long)\n",
    "        eps = torch.rand_like(x0)\n",
    "        xt = self.q_sample(x0,t)\n",
    "        eps_pred = self.p_sample(xt,t)\n",
    "        loss = self.loss_fn(eps_pred,eps)\n",
    "        self.log('val_mse',loss)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-6,weight_decay=1e-12)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "timesteps = 250\n",
    "eps_model = EPS_Model(timesteps,dev='cpu')\n",
    "diffusion_model = DiffusionModel(timesteps,eps_model,dev='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(name='Diffusion',project='Amex')\n",
    "logger.watch(diffusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(save_top_k=3,monitor='val_mse',mode='min',dirpath='Models',filename='Diffusion-Model-{epoch:04d}-{val_mse:.10f}-{val_acc:.5f}')\n",
    "trainer = pl.Trainer(max_epochs=20000,logger=logger,callbacks=[checkpoint],accelerator='gpu',limit_train_batches=5000,limit_val_batches=1500,resume_from_checkpoint='Models/Diffusion-Model-epoch=3003-val_mse=0.9410688280-val_acc=0.49567.ckpt')\n",
    "trainer.fit(model=diffusion_model, train_dataloaders=train_dataloader,val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![Screenshot-2022-10-22-at-10-34-45-AM.png](https://i.postimg.cc/qqV2Xcqm/Screenshot-2022-10-22-at-10-34-45-AM.png)](https://postimg.cc/0Mn69wpm) |\n",
    "| :---: |\n",
    "| *The validation error even after about 4000 epochs was too high. It might have converged if given more training time. Due to time constraints we stopped it to explore other things. Nevertheless we had fun working on it!*|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess().fit(df_X)\n",
    "X = preprocess.transform(df_X)\n",
    "y = df_y.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models\n",
    "* We tried different tree based classifier models and the performances were almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier: \t 0.8101797072647757\n",
      "Gradient Boosting Classifier: \t 0.8154879029358506\n",
      "Histogram based Gradient Boosting Classifier: \t 0.8142698071311991\n",
      "Extra Trees Classifier: \t 0.8057758224325743\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "print(f'Random Forest Classifier: \\t {cross_val_score(clf,X,y,n_jobs=3,cv=3).mean()}')\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=200)\n",
    "print(f'Gradient Boosting Classifier: \\t {cross_val_score(clf,X,y,n_jobs=3,cv=3).mean()}')\n",
    "\n",
    "clf = HistGradientBoostingClassifier()\n",
    "print(f'Histogram based Gradient Boosting Classifier: \\t {cross_val_score(clf,X,y,n_jobs=3,cv=3).mean()}')\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=200)\n",
    "print(f'Extra Trees Classifier: \\t {cross_val_score(clf,X,y,n_jobs=3,cv=3).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "* XGBoost (Extreme Gradient boosting) is a highly efficient, regularised implementation of Gradient Boosted Trees.\n",
    "\n",
    "> XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable - Official Documentation\n",
    "\n",
    "\n",
    "References:\n",
    "* https://xgboost.readthedocs.io/en/stable/\n",
    "* https://www.geeksforgeeks.org/xgboost/\n",
    "* https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "\n",
    "|![DAEs](https://s1.gifyu.com/images/image4368fb1ff45e09af.png)|\n",
    "|:--:|\n",
    "| *Flowchart of XGBoost,Degradation state recognition of piston pump based on ICEEMDAN and XGBoost by Rui Guo*|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost Classifier: \t 0.8147557380157023\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier() # Default parameters\n",
    "sc = cross_val_score(clf,X,y,n_jobs=3,cv=3).mean()\n",
    "clear_output()\n",
    "print(f'Xgboost Classifier: \\t {sc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model\n",
    "* LGBM is a gradient boosting model that uses decision trees to increases the efficiency of the model and reduces memory usage. \n",
    "* The Model uses gradient-based one side sampling and exclusive feature bundling (EFB) in order to address the constraints of histogram-based approach  which is largely employed in all GBDT models.\n",
    "* In contrast to previous boosting algorithms that develop trees level-by-level, LightGBM divides the tree leaf-wise. It selects the leaf with the greatest delta loss for growth.\n",
    "* The leaf-wise algorithm has less loss than the level-wise algorithm since the leaf is fixed.\n",
    "* The complexity of the model could rise as a result of leaf-wise tree growth, which could also result in overfitting in limited samples.\n",
    "\n",
    "References:\n",
    "* https://lightgbm.readthedocs.io/en/v3.3.2/\n",
    "* https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/\n",
    "* https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "\n",
    "\n",
    "|![DAEs](https://s1.gifyu.com/images/imagecdb5e6b33277ba69.png)|\n",
    "|:--:|\n",
    "| *How Light GBM Models work, What is LightGBM, How to implement it? How to fine tune the parameters? by Pushkar Mandot*|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Classifier: \t 0.8154813657490635\n"
     ]
    }
   ],
   "source": [
    "clf = LGBMClassifier() # Default parameters\n",
    "sc = cross_val_score(clf,X,y,n_jobs=3,cv=3).mean() # Fastest fit at 2.5 seconds per model!\n",
    "clear_output()\n",
    "print(f'LGBM Classifier: \\t {sc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cat Boost Classifier\n",
    "\n",
    "* CatBoost is a high-performance open source library for gradient boosting on decision trees. \n",
    "* It produces cutting-edge results without the substantial data training that other machine learning techniques normally demand, and it offers strong out-of-the-box support for the more descriptive data formats that go along with many commercial problems.\n",
    "* In order to limit the number of features split per level to one, CatBoost uses oblivious decision trees (binary trees in which the same features are utilised to make left and right split for each level of the tree). This reduces prediction time.\n",
    "\n",
    "References:\n",
    "* https://www.geeksforgeeks.org/catboost-ml/\n",
    "* https://catboost.ai/\n",
    "* https://www.aaai.org/Papers/Workshops/1994/WS-94-01/WS94-01-020.pdf\n",
    "\n",
    "\n",
    "|![DAEs](https://s1.gifyu.com/images/image2ac02d57d53c7f9e.png)|\n",
    "|:--:|\n",
    "| *Improving supervised learning by Feature Decomposition, Oded Maimon and Lior Rokach*|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier: \t 0.8162200678559989\n"
     ]
    }
   ],
   "source": [
    "clf = CatBoostClassifier() # Default parameters\n",
    "sc = cross_val_score(clf,X,y,n_jobs=3,cv=3).mean() \n",
    "clear_output()\n",
    "print(f'CatBoost Classifier: \\t {sc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet\n",
    "https://arxiv.org/abs/1908.07442\n",
    "* The core idea behind TabNet is to apply deep neural networks to tabular data.\n",
    "* The motivation behind applying deep neural networks to tabular data is that they performed really good with other domains like text, speech, etc. And tree-based models dont efficiently learn to reduce the error unlike deep neural networks.\n",
    "* TabNet uses a technique known as the sequential attention mechanism to enable the selection of the feature that will result in high interpretability and effective training.\n",
    "\n",
    "|![DAEs](https://s1.gifyu.com/images/image488c30aca026ecce.png)|\n",
    "|:--:|\n",
    "| *TabNet Model Architecture. Image by Adam Shafi. Inspired by https://arxiv.org/pdf/1908.07442.pdf.*|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Classifier: \t 0.8139865264819454\n"
     ]
    }
   ],
   "source": [
    "kfolds = StratifiedKFold()\n",
    "scores = []\n",
    "for train_index,test_index in kfolds.split(X,y):\n",
    "    clf = TabNetClassifier(verbose=3)\n",
    "    clf.fit(X.iloc[train_index].values,y.iloc[train_index].values,eval_set=[(X.iloc[test_index].values,y.iloc[test_index].values)])\n",
    "    yhat = clf.predict(X.iloc[test_index].values)\n",
    "    scores.append( accuracy_score(y.iloc[test_index],yhat) )\n",
    "sc = sum(scores)/len(scores)\n",
    "clear_output()\n",
    "print(f'TabNet Classifier: \\t {sc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D- CNN ResNet style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1d(in_f,out_f,kernel_size,padding='same',**paras):\n",
    "    return nn.Sequential(nn.Conv1d(in_f,out_f,kernel_size,padding=padding,**paras),nn.BatchNorm1d(out_f),nn.ReLU())\n",
    "    \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self,in_f,k):\n",
    "        super().__init__()\n",
    "        self.C1 = Conv1d(in_f,in_f,k)\n",
    "        self.C2 = Conv1d(in_f,in_f,k)\n",
    "        self.C3 = Conv1d(in_f,in_f,k)\n",
    "    def forward(self,inpt):\n",
    "        x = self.C1(inpt)\n",
    "        x = self.C2(x)\n",
    "        x = self.C3(x)\n",
    "        x = x + inpt\n",
    "        return x\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self,in_f,k):\n",
    "        super().__init__()\n",
    "        self.C1 = Conv1d(in_f,4*in_f,k)\n",
    "        self.M = nn.MaxPool1d(4,4)\n",
    "    def forward(self,x):\n",
    "        x = self.C1(x)\n",
    "        x = self.M(x)\n",
    "        return x\n",
    "\n",
    "class ResNet_1D(pl.LightningModule):\n",
    "    def __init__(self,class_weights):\n",
    "        super().__init__()\n",
    "        self.L1 = nn.Linear(215,512) # B * 512\n",
    "        self.C1 = Conv1d(1,4,kernel_size=128) # B * 1 * 512\n",
    "\n",
    "        self.R1 = ResnetBlock(4,96) # B * 4 * 512\n",
    "        self.D1 = DownBlock(4,96) # B * 4 * 128\n",
    "\n",
    "        self.R2 = ResnetBlock(16,64) # B * 16 * 128\n",
    "        self.D2 = DownBlock(16,64) # B * 16 * 128\n",
    "\n",
    "        self.R3 = ResnetBlock(64,16) # B * 64 * 32\n",
    "        self.D3 = DownBlock(64,16) # B * 64 * 32\n",
    "\n",
    "        self.R4 = ResnetBlock(256,2) # B * 256 * 8\n",
    "        self.D4 = nn.MaxPool1d(4,4) # B * 256 * 8\n",
    "\n",
    "\n",
    "        self.L2 = nn.Linear(512,32) # B *  512\n",
    "        self.L3 = nn.Linear(32,4) \n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight = class_weights) \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.L1(x))\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.C1(x)\n",
    "\n",
    "        x = self.D1(self.R1(x))\n",
    "        x = self.D2(self.R2(x))\n",
    "        x = self.D3(self.R3(x))\n",
    "        x = self.D4(self.R4(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0],512)\n",
    "        x = torch.relu(self.L2(x))\n",
    "        x = torch.sigmoid(self.L3(x))\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = self.forward(x)\n",
    "        loss = self.loss_fn(yhat,y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = (y.argmax(1) == yhat.argmax(1)).float().sum() / len(y)\n",
    "        self.log(\"train_acc\",acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = self.forward(x)\n",
    "        loss = self.loss_fn(yhat,y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = (y.argmax(1) == yhat.argmax(1)).float().sum() / len(y)\n",
    "        self.log(\"val_acc\",acc)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![Screenshot-2022-10-22-at-6-49-08-PM.png](https://i.postimg.cc/vB3jrYZH/Screenshot-2022-10-22-at-6-49-08-PM.png)](https://postimg.cc/Vr0D1ct3) |\n",
    "| :---: |\n",
    "| *Training Logs for the model, on submission this model archieved ~0.81 score* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D CNN with Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.BN = nn.BatchNorm1d(1)\n",
    "        self.C1 = nn.Conv1d(1,4,7,padding='same',padding_mode='reflect')\n",
    "        self.C2 = nn.Conv1d(4,1,5,padding='same',padding_mode='reflect')\n",
    "        self.L1 = nn.Linear(195,64)\n",
    "        self.L2 = nn.Linear(64,195)\n",
    "    def forward(self,x):\n",
    "        x = self.BN(x)\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        x = torch.relu(self.L1(x))\n",
    "        x = torch.sigmoid(self.L2(x))\n",
    "        return x\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.BN1 = nn.BatchNorm1d(1)\n",
    "        self.BN2 = nn.BatchNorm1d(32)\n",
    "        self.C1 = nn.Conv1d(1,4,7,2)\n",
    "        self.C2 = nn.Conv1d(4,8,5,2)\n",
    "        self.C3 = nn.Conv1d(8,16,3,2)\n",
    "        self.C4 = nn.Conv1d(16,32,1,2)\n",
    "        self.L1 = nn.Linear(352,64)\n",
    "        self.L2 = nn.Linear(64,4)\n",
    "        self.Attention = AttentionModule()\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)\n",
    "        a = self.Attention(x)\n",
    "        x = self.BN1(a*x)\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        x = self.C3(x)\n",
    "        x = self.C4(x)\n",
    "        x = self.BN2(x)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = torch.relu(self.L1(x))\n",
    "        x = torch.sigmoid(self.L2(x))\n",
    "        return x\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = self.forward(x)\n",
    "        loss = nn.functional.binary_cross_entropy(yhat,y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = (y.argmax(1) == yhat.argmax(1)).float().sum() / len(y)\n",
    "        self.log(\"train_acc\",acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = self.forward(x)\n",
    "        loss = nn.functional.binary_cross_entropy(yhat,y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = (y.argmax(1) == yhat.argmax(1)).float().sum() / len(y)\n",
    "        self.log(\"val_acc\",acc)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |[![Screenshot-2022-10-24-at-4-01-50-PM.png](https://i.postimg.cc/50GVVs9n/Screenshot-2022-10-24-at-4-01-50-PM.png)](https://postimg.cc/ygF5hXZZ)|\n",
    " |:---:|\n",
    " | *1-D CNN with attention* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring the problem to a computer vision problem\n",
    "* https://www.biorxiv.org/content/10.1101/2020.05.02.074203v1.full\n",
    "* This is a novel method by which the tabular problem can be refactored into a Computer Vision problem.\n",
    "* Each row of the tabular data is reshaped into a kernel and is applied on a standard image. \n",
    "* Pretrained models like resnet is finetuned on the transformed images to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_binary_cross_entropy(logits, targets, gamma=2):\n",
    "    p = logits.reshape(-1)\n",
    "    t = targets.reshape(-1)\n",
    "    p = torch.where(t >= 0.5, p, 1-p)\n",
    "    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n",
    "    loss = logp*((1-p)**gamma)\n",
    "    loss = 4*loss.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "class CV_RESNET(pl.LightningModule):\n",
    "    def __init__(self,std_img,loss):\n",
    "        super().__init__()\n",
    "        self.L1 =  nn.Linear(215,225)\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = torch.nn.Sequential(nn.Linear(num_features, 32),nn.Linear(32, 4),nn.Sigmoid())\n",
    "        # self.class_weights = class_weights\n",
    "        self.resnet.to(device)\n",
    "        self.loss_fn = loss\n",
    "        self.std_img = std_img\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.L1(x))\n",
    "        kernels = x.reshape(-1,3,3,5,5)\n",
    "        x = self.Conv(self.std_img,kernels)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def Conv(self,img,kernels):\n",
    "        return torch.cat([nn.functional.conv2d(img,kernel,padding='same') for kernel in kernels])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = self.forward(x)\n",
    "        loss = self.loss_fn(yhat,y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = (y.argmax(1) == yhat.argmax(1)).float().sum() / len(y)\n",
    "        self.log(\"train_acc\",acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        yhat = self.forward(x)\n",
    "        loss = self.loss_fn(yhat,y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = (y.argmax(1) == yhat.argmax(1)).float().sum() / len(y)\n",
    "        self.log(\"val_acc\",acc)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| [![Screenshot-2022-10-22-at-7-03-15-PM.png](https://i.postimg.cc/xTzP3fsj/Screenshot-2022-10-22-at-7-03-15-PM.png)](https://postimg.cc/HVT5pgDF) |\n",
    "| :---: |\n",
    "| *Training logs for the model* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling all predictions\n",
    "* We used voting ensemble on all the models mentioned above to make a submission with score 0.823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Stacking with 10-fold ensembles (best submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![](https://www.researchgate.net/publication/341158231/figure/fig2/AS:887818180112386@1588683757703/The-1010-fold-cross-validation-model-used-as-a-Base-learner-in-this-study.jpg) |\n",
    "| :---: |\n",
    "| Each model in the entire stack is a 10 Fold Ensemble. Credits: *DOI:10.3390/sym12050728* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://i.postimg.cc/FzWvNVjV/autogluon.png\">\n",
    "<img src=\"https://i.postimg.cc/mZRg3dkJ/autogluon.png\" width=100% height=auto>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We used 10 different models along with weighted mean to create a 3 level stack along with an output level\n",
    "* Each of the 10 models are folded ensembles with 10 folds.\n",
    "* The models used are:\n",
    "    * Random Forest with entropy as criterion\n",
    "    * Random Forest with gini entropy as criterion\n",
    "    * Extra Trees with entropy\n",
    "    * Extra Trees with gini entropy\n",
    "    * XGBoost\n",
    "    * CatBoost\n",
    "    * Light GBM\n",
    "    * Light GBM Large\n",
    "    * Light GBM with extra trees\n",
    "    * Fast AI TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install autogluon\n",
    "\n",
    "\n",
    "train_data = TabularDataset(pd.concat([X,y],axis=1))\n",
    "\n",
    "predictor = TabularPredictor(label='Default_Flag',path='AutogluonModels/FinalRun',learner_kwargs={'ignored_columns':['ID']}).fit(train_data=train_data,\n",
    "                            presets='best_quality',\n",
    "                            excluded_model_types=['KNN'],\n",
    "                            ag_args_fit={'num_gpus': 1},verbosity=4,num_bag_folds=10,\n",
    "                             num_stack_levels=3,\n",
    "                             unlabeled_data=X_val\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TabularPredictor.load('AutogluonModels/FinalRun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Top 15 Models\n",
    "                      model  score_val  pred_time_val      fit_time\n",
    "0       WeightedEnsemble_L4   0.819208     511.186869  21881.425800     \n",
    "1       WeightedEnsemble_L5   0.819025     625.097327  23095.855794     \n",
    "2            XGBoost_BAG_L4   0.818970     591.500129  22406.417115     \n",
    "3         LightGBMXT_BAG_L4   0.818950     587.146719  22572.019165     \n",
    "4           LightGBM_BAG_L4   0.818926     586.659701  22461.712277         \n",
    "5           LightGBM_BAG_L3   0.818846     391.771510  15924.713362         \n",
    "6         LightGBMXT_BAG_L3   0.818833     391.979788  15966.168671         \n",
    "7           CatBoost_BAG_L4   0.818828     586.273718  22247.273773         \n",
    "8       WeightedEnsemble_L3   0.818815     261.605411  11844.286081         \n",
    "9           CatBoost_BAG_L3   0.818778     391.409762  15710.005783         \n",
    "10           XGBoost_BAG_L3   0.818772     396.745541  15828.057830         \n",
    "11   NeuralNetFastAI_BAG_L3   0.818702     404.406859  17730.750722         \n",
    "12        LightGBMXT_BAG_L2   0.818700     202.036779   9208.895716         \n",
    "13           XGBoost_BAG_L2   0.818672     206.616195   9080.980620      \n",
    "14          LightGBM_BAG_L2   0.818584     201.759345   9152.735864  \n",
    "15          CatBoost_BAG_L2   0.818556     201.324706   8911.695362\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_allx.csv')\n",
    "df = preprocess.transform(df)\n",
    "df = TabularDataset(df)\n",
    "preds = Model.predict(df)\n",
    "final ={'ID':df['ID'],'1':preds}\n",
    "pd.DataFrame(final).to_csv('DSC_Thunder_BUDDIES_UGHF2493_test_allx.csv',header=False,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "337a5056b4ea588e42a0b9cc0fc65a42b3e5f80956a0e8a5dcee6ced80cea451"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
